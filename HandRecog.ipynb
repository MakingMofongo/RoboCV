{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize mediapipe\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['okay', 'peace', 'thumbs up', 'thumbs down', 'call me', 'stop', 'rock', 'live long', 'fist', 'smile']\n"
     ]
    }
   ],
   "source": [
    "# Load the gesture recognizer model\n",
    "model = load_model('mp_hand_gesture')\n",
    "# Load class names\n",
    "f = open('gesture.names', 'r')\n",
    "classNames = f.read().split('\\n')\n",
    "f.close()\n",
    "print(classNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pop from an empty set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abdul Rasheed\\OneDrive\\Documents\\GitHub\\RoboCV\\HandRecog.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abdul%20Rasheed/OneDrive/Documents/GitHub/RoboCV/HandRecog.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m       mpDraw\u001b[39m.\u001b[39mdraw_landmarks(frame, handslms, mpHands\u001b[39m.\u001b[39mHAND_CONNECTIONS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abdul%20Rasheed/OneDrive/Documents/GitHub/RoboCV/HandRecog.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Predict gesture in Hand Gesture Recognition project\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Abdul%20Rasheed/OneDrive/Documents/GitHub/RoboCV/HandRecog.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict([landmarks])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abdul%20Rasheed/OneDrive/Documents/GitHub/RoboCV/HandRecog.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(prediction)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abdul%20Rasheed/OneDrive/Documents/GitHub/RoboCV/HandRecog.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m classID \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(prediction)\n",
      "File \u001b[1;32mc:\\Users\\Abdul Rasheed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1710\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1704\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   1705\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mUsing Model.predict with \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1706\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1707\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1708\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1710\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   1711\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1712\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1713\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   1714\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   1715\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1716\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1717\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1718\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1719\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1720\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   1722\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\Abdul Rasheed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1394\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1392\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1393\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1394\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Abdul Rasheed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1149\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1146\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution_value \u001b[39m=\u001b[39m steps_per_execution\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1148\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1150\u001b[0m     x,\n\u001b[0;32m   1151\u001b[0m     y,\n\u001b[0;32m   1152\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1153\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1154\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1155\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1156\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1157\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1158\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1159\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1160\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mds_context\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1161\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   1163\u001b[0m strategy \u001b[39m=\u001b[39m ds_context\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Abdul Rasheed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:253\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m (sample_weights, _, _) \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39mhandle_partial_sample_weights(\n\u001b[0;32m    249\u001b[0m     y, sample_weights, sample_weight_modes, check_all_flat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    251\u001b[0m inputs \u001b[39m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[1;32m--> 253\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m(\u001b[39mint\u001b[39;49m(i\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m nest\u001b[39m.\u001b[39;49mflatten(inputs))\u001b[39m.\u001b[39;49mpop()\n\u001b[0;32m    254\u001b[0m _check_data_cardinality(inputs)\n\u001b[0;32m    256\u001b[0m \u001b[39m# If batch_size is not passed but steps is, calculate from the input data.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[39m# Default to 32 for backwards compat.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pop from an empty set'"
     ]
    }
   ],
   "source": [
    "# Initialize the webcam for Hand Gesture Recognition Python project\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "  # Read each frame from the webcam\n",
    "  _, frame = cap.read()\n",
    "\n",
    "  x , y, c = frame.shape\n",
    "\n",
    "  # Flip the frame vertically\n",
    "  frame = cv2.flip(frame, 1)\n",
    "\n",
    "  \n",
    "  framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "  # Get hand landmark prediction\n",
    "  result = hands.process(framergb)\n",
    "  className = ''\n",
    "\n",
    "\n",
    "  landmarks=[]\n",
    "  #post process\n",
    "  if result.multi_hand_landmarks:\n",
    "    landmarks = []\n",
    "    for handslms in result.multi_hand_landmarks:\n",
    "        for lm in handslms.landmark:\n",
    "       \n",
    "            # print(id, lm)\n",
    "            lmx = int(lm.x * x)\n",
    "            lmy = int(lm.y * y)\n",
    "            landmarks.append([lmx, lmy])\n",
    "        # Drawing landmarks on frames\n",
    "        mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "  # # Predict gesture in Hand Gesture Recognition project\n",
    "  # prediction = model.predict([landmarks])\n",
    "  # print(prediction)\n",
    "  # classID = np.argmax(prediction)\n",
    "  # className = classNames[classID]\n",
    "  # # show the prediction on the frame\n",
    "  # cv2.putText(frame, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2, cv2.LINE_AA)\n",
    "\n",
    "  \n",
    "  # Show the final output\n",
    "  cv2.imshow(\"Output\", frame)\n",
    "  if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if result.multi_hand_landmarks:\n",
    "    print('ye')\n",
    "    landmarks = []\n",
    "    for handslms in result.multi_hand_landmarks:\n",
    "        for lm in handslms.landmark:\n",
    "       \n",
    "            # print(id, lm)\n",
    "            lmx = int(lm.x * x)\n",
    "            lmy = int(lm.y * y)\n",
    "            landmarks.append([lmx, lmy])\n",
    "        # Drawing landmarks on frames\n",
    "        mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e911bb79b95f2b40b7952d718cdc0528e6543e46ce2458b7a9454f8f1b299696"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
